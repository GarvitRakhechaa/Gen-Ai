from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import torch 
import torch.nn as nn
from torch.optim import AdamW
from dotenv import load_dotenv
load_dotenv("../.env")
Hf_Token =  os.getenv('HF_TOKEN')


# get a hugging face token
os.environ["HF_TOKEN"] = Hf_Token


# # pulled model from hugging face
model_name = "google/gemma-3-1b-it"


#checkd gpu 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)  # loaded google gemma tokenizer



# chat template
input_conversation = [
    {"role": "user", "content": "which is best place to learn Gen AI "},
    {"role": "assistant", "content": "the best place to learn Gen AI is "},
]

input_tokens_tensor = tokenizer.apply_chat_template(  
    conversation = input_conversation,
    tokenize = True, 
    return_tensors="pt"
)


input_detokens = tokenizer.apply_chat_template(
    conversation = input_conversation,
    tokenize = False,
    continue_final_message=True,
)

# # print(input_detokens)

output_label = "Gen AI 1.0 by ChaiCode and Piyush Garg"
full_conversation = input_detokens + output_label + tokenizer.eos_token

print(" full conversation is ",full_conversation)

input_tokenized = tokenizer(full_conversation ,return_tensors="pt", add_special_tokens=False)
print("full conversation tokens ",input_tokenized)
# # # created model

input_ids = input_tokenized["input_ids"][:, :-1]
target_ids = input_tokenized["input_ids"][:, 1:]
print(f"input_ids : {input_ids}")
print(f"target_ids : {target_ids}")

def calculate_loss(logits, labels):
    loss_fn = nn.CrossEntropyLoss(reduction="none")
    cross_entropy = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))
    return cross_entropy


model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    attn_implementation='eager'
)

model.train()

optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)

for _ in range(10):
    out = model(input_ids = input_ids)
    loss = calculate_loss(out.logits, target_ids).mean()
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    print("loss: ",loss.item())

# # generated output tokens using model
output_tokens_tensor = model.generate(input_tokens_tensor)  
print("ye output tokens hai ",output_tokens_tensor)

# # detokenized tokens generated by model
detokenized_tensor = tokenizer.batch_decode(output_tokens_tensor)  # detokenizing tokens generatedc by model


print("ye detokenized output hai ",detokenized_tensor)

# # # prepare dataset
# "data set and this will be put in model"
# "input : THe best place to learn Gen AI is"
# "Output : best place to learn Gen AI is ChaiCode Cohort"



